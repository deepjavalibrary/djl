import java.util.zip.GZIPInputStream

group "ai.djl.onnxruntime"

dependencies {
    api project(":api")
    api "com.microsoft.onnxruntime:onnxruntime:${onnxruntime_version}"

    testImplementation project(":testing")
    testImplementation project(":engines:pytorch:pytorch-engine")
    testImplementation project(":extensions:tokenizers")

    testRuntimeOnly "org.slf4j:slf4j-simple:${slf4j_version}"
}

processResources {
    def basePath = "${project.projectDir}/build/resources/main/nlp"
    outputs.dir file(basePath)
    doLast {
        def url = "https://mlrepo.djl.ai/model/nlp"
        def tasks = [
                "fill_mask",
                "question_answer",
                "text_classification",
                "text_embedding",
                "token_classification",
        ]

        for (String task : tasks) {
            def file = new File("${basePath}/${task}/ai.djl.huggingface.onnxruntime.json")
            if (file.exists()) {
                project.logger.lifecycle("model zoo metadata alrady exists: ${task}")
            } else {
                project.logger.lifecycle("Downloading model zoo metadata: ${task}")
                file.getParentFile().mkdirs()
                def downloadPath = new URL("${url}/${task}/ai/djl/huggingface/onnxruntime/models.json.gz")
                downloadPath.withInputStream { i -> file.withOutputStream { it << new GZIPInputStream(i) } }
            }
        }
    }
}

publishing {
    publications {
        maven(MavenPublication) {
            artifactId "onnxruntime-engine"
            pom {
                name = "DJL Engine Adapter for ONNX Runtime"
                description = "Deep Java Library (DJL) Engine Adapter for ONNX Runtime"
                url = "http://www.djl.ai/engines/onnxruntime/${project.name}"
            }
        }
    }
}
